{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80354-7edd-4403-9c8b-97a4712f4dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain[all]\n",
    "!pip install sagemaker --upgrade\n",
    "!pip install  boto3\n",
    "!pip install requests_aws4auth\n",
    "!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b216f11-a2b5-4f0b-b4de-060a1a64076a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## initial sagemaker env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19f05414-d44b-464e-a0c7-46c317378ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::687912291502:role/webui-notebook-stack-ExecutionRole-62U5FV4LJQS\n",
      "sagemaker bucket: sagemaker-us-west-2-687912291502\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3aed78-e14b-4d30-93d3-2871d6603bd8",
   "metadata": {},
   "source": [
    "## intial lanchain lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4aae9f94-a8df-49b5-b22b-f89e8d756b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"]= \"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\"\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "hostname=\"vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com\"\n",
    "region='us-west-2'\n",
    "username=\"admin\"\n",
    "passwd=\"(OL>0p;/\"\n",
    "index=\"qa_index\"\n",
    "size=10\n",
    "\n",
    "class EmbeddingContentHandler(ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"vectors\"]\n",
    "\n",
    "embedding_content_handler = EmbeddingContentHandler()\n",
    "sm_embeddings = SagemakerEndpointEmbeddings(\n",
    "    # endpoint_name=\"endpoint-name\", \n",
    "    # credentials_profile_name=\"credentials-profile-name\", \n",
    "    #endpoint_name=\"huggingface-textembedding-bloom-7b1-fp1-2023-04-17-03-31-12-148\", \n",
    "    endpoint_name=\"st-paraphrase-mpnet-base-v2-2023-04-17-10-05-10-718-endpoint\",\n",
    "    region_name=\"us-west-2\", \n",
    "    content_handler=embedding_content_handler\n",
    ")\n",
    "\n",
    "\n",
    "class TextGenContentHandler(ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({prompt: prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "text_gen_content_handler = TextGenContentHandler()\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=\"bloomz-7b1-mt-2023-04-17-02-49-58-645-endpoint\", \n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs={\"temperature\":1e-10},\n",
    "        content_handler=text_gen_content_handler\n",
    ")\n",
    "\n",
    "opensearch_vector_search = OpenSearchVectorSearch(\n",
    "    \"https://\"+hostname+\":443\",\n",
    "    index,\n",
    "    sm_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd74903-b3ba-4ed3-a019-d1acfbb2c054",
   "metadata": {},
   "source": [
    "## func(for local test ) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0819cc2-5be1-4f78-8826-90c3ff2c72d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "source_includes = [\"question\",\"answer\"]\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "\n",
    "########parse k-NN search resule###############\n",
    "# input:\n",
    "#  r: AOS returned json\n",
    "# return:\n",
    "#  result : array of topN text  \n",
    "#############################################\n",
    "def parse_results(r):\n",
    "    res = []\n",
    "    result = []\n",
    "    for i in range(len(r['hits']['hits'])):\n",
    "        h = r['hits']['hits'][i]\n",
    "        if h['_source']['question'] not in clean:\n",
    "          result.append(h['_source']['question'])\n",
    "          res.append('<第'+str(i+1)+'条信息>'+h['_source']['question'] + '。</第'+str(i+1)+'条信息>\\n')\n",
    "    print(res)\n",
    "    return result\n",
    "\n",
    "########get embedding vector by SM llm########\n",
    "# input:\n",
    "#  questions:question texts(list)\n",
    "#  sm_client: sagemaker runtime client\n",
    "#  sm_endpoint:Sagemaker embedding llm endpoint\n",
    "# return:\n",
    "#  result : vector of embeded text  \n",
    "#############################################\n",
    "def get_vector_by_sm_endpoint(questions,sm_client,endpoint_name,parameters):\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                Body=json.dumps(\n",
    "                {\n",
    "                    \"inputs\": questions,\n",
    "                    \"parameters\": parameters\n",
    "                }\n",
    "                ),\n",
    "                ContentType=\"application/json\",\n",
    "            )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "########get embedding vector by lanchain vector search########\n",
    "# input:\n",
    "#  questions:question texts(list0\n",
    "#  embedings:lanchain embeding models\n",
    "# return:\n",
    "#  result : vector of embeded text  \n",
    "#############################################################\n",
    "def get_vector_by_lanchain(questions , embedings):\n",
    "    doc_results = embeddings.embed_documents(questions)\n",
    "    print(doc_results[0])\n",
    "    return doc_results\n",
    "\n",
    "\n",
    "########k-nn search by lanchain########\n",
    "# input:\n",
    "#  q:question text\n",
    "#  vectorSearch: lanchain VectorSearch instance\n",
    "# return:\n",
    "#  result : k-NN search result  \n",
    "#############################################################\n",
    "def search_using_lanchain(question, vectorSearch):\n",
    "  docs = vectorSearch.similarity_search(query)\n",
    "  return docs\n",
    "\n",
    "\n",
    "\n",
    "########k-nn by native AOS########\n",
    "# input:\n",
    "#  q_embedding:embeded question text(array)\n",
    "#  index:AOS k-NN index name\n",
    "#  hostname: aos endpoint\n",
    "#  username: aos username\n",
    "#  passwd: aos password\n",
    "#  source_includes: fields to return\n",
    "#  k: topN\n",
    "# return:\n",
    "#  result : k-NN search result  \n",
    "#############################################################\n",
    "def search_using_aos_knn(q_embedding, hostname, username,passwd, index, source_includes, size):\n",
    "    print(1, q)\n",
    "    awsauth = (username, passwd)\n",
    "    query = {\n",
    "          \"size\": num_output,\n",
    "          \"_source\": {\n",
    "            \"includes\": source_includes\n",
    "          },\n",
    "          \"size\": size,\n",
    "          \"query\": {\n",
    "            \"knn\": {\n",
    "              \"sentence_vector\": {\n",
    "                \"vector\": q_embedding,\n",
    "                \"k\": size\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    r = requests.post(hostname + index + '/_search', auth=awsauth, headers=headers, json=query)\n",
    "    return r.text\n",
    "\n",
    "\n",
    "\n",
    "########k-nn ingestion by native AOS########\n",
    "# input:\n",
    "#  docs:ingestion source documents\n",
    "#  index:AOS k-NN index name\n",
    "#  hostname: aos endpoint\n",
    "#  username: aos username\n",
    "#  passwd: aos password\n",
    "# return:\n",
    "#  result : N/A  \n",
    "#############################################################\n",
    "def k_nn_ingestion_by_aos(docs,index,hostname,username,passwd):\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "         hosts = [{'host': hostname, 'port': 443}],\n",
    "         ##http_auth = awsauth ,\n",
    "         http_auth = auth ,\n",
    "         use_ssl = True,\n",
    "         verify_certs = True,\n",
    "         connection_class = RequestsHttpConnection\n",
    "     )\n",
    "    for doc in docs:\n",
    "        vector_field = doc['sentence_vector']\n",
    "        question_filed = doc['question']\n",
    "        answer = doc['answer']\n",
    "        document = { \"question\": question_filed, 'answer':answer_field, \"sentence_vector\": vector_field} \n",
    "        search.index(index=index, body=document) \n",
    "\n",
    "\n",
    "########k-nn ingestion by lanchain #########################\n",
    "# input:\n",
    "#  docs:ingestion source documents\n",
    "#  vectorStore: lanchain AOS vectorStore instance\n",
    "# return:\n",
    "#  result : N/A  \n",
    "#############################################################\n",
    "def k_nn_ingestion_by_lanchain(docs,vectorStore):\n",
    "    for doc in docs:\n",
    "        opensearch_vector_search.add_texts(docs,batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa302e0a-b8ed-4fa0-934e-ab4ff0740316",
   "metadata": {
    "tags": []
   },
   "source": [
    "## major chain pipeline ################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ef73a-d691-4d30-b35e-1a5254cdb8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1: data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9bb9c564-9d44-4ba3-a279-34b6df2aa3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_question = \"\"\"在中国区是否可用？\n",
    "为什么在合成的小数据集上第一次查询的时候需要几分钟才返回？\n",
    "能支持多大规模数据的查询？查询速度怎么样？\n",
    "AWS Clean Rooms 是如何计费的？\n",
    "AWS Clean Rooms 从哪里可以看到CRPU-hours的用量？\n",
    "目前可以支持什么数据源的接入？ \n",
    "一个协作中，最大的并发查询数是多少？\n",
    "我们如何说服客户相信洁净室的安全性和正确性？我在这里的大多数合规计划中都看不到Clean Rooms， https://aws.amazon.com/compliance/services-in-scope/ ？\n",
    "数据源必须在AWS上么？\n",
    "数据是如何进入到S3？\n",
    "这些安全控制权限只是作用与分析么？能够改动它方的数据么？\n",
    "协作方的数据会移动么？协作方的原始数据会集中到Clean rooms 吗？\n",
    "是否发起者和数据贡献者都会被收费？\n",
    "数据贡献方的S3， 会产生API会产生调用次数收费么？\n",
    "是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？\n",
    "如果已经在用Athena，S3桶中已经有数据了，是否能基于S3中这个数据就地加入AWS Clean Room?\n",
    "是否能通过SDK也就是代码来调用Clean room的联合分析\n",
    "在输出分析结果的时候，能否按照字段进行分区？\n",
    "最大的参与方是多少？ 如果超过了限制怎么办？\n",
    "AWS clean rooms 用的什么加密算法？\n",
    "加密过程中有密文落地么？如果有，密文存在哪里？\n",
    "数据上传到S3的过程中，有两种加密方式Server Side 和 Client Side加密，如果客户的安全等级比较高，在数据上传之前做了加密，再想交给clean room 去处理，之前提到的C3R的加密方式，具体是一个怎么样的流程呢？\n",
    "AWS Clean Room在安全计算方面，应该归属哪一类？\n",
    "是否所有的字段都可以进行加密？\n",
    "如果要对某个字段进行求和，求平均的数据计算，是否可以加密？\n",
    "如果想要通过某些字段进行where过滤，这些字段应该是什么类型？\n",
    "C3R客户端是否有实现任何non-standard的加密算法?\n",
    "AWS Clean rooms 与 AMC的区别与联系是什么？\n",
    "AWS Clean rooms 与 其他的clean room服务商的区别？\n",
    "有哪些典型的应用场景？\n",
    "AWS Clean Rooms 的 data catalog 是如何实现的？ data sharing permission 是如何实现的？\n",
    "可以在哪些地方进行Clean room的联合分析？\n",
    "数据提供方如果对联合分析的收益方进行收费，或者实现一个数据授权的合同？\n",
    "AWS Clean Rooms 与 AWS Data Exchange 是什么关系？\n",
    "AWS Clean Rooms中是否支持视图？\n",
    "如果数据合作方没有aws account，能否支持？\n",
    "是否能够支持这个协作中，仅仅允许指定运行固定的SQL？\n",
    "AWS Clean Rooms可以让数据贡献者提供一些样例数据进行预览么？\n",
    "当一个数据贡献者的数据发生更新后会怎么样？\n",
    "AWS Clean Rooms 未来有哪些前进的方向？\n",
    "Service Team 有哪些相关的同事\n",
    "\"\"\"\n",
    "questions = all_question.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "565ebb18-ac48-4e27-8e86-a49cd221398f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_answer = \"\"\"目前没有落地中国区的时间表，已经在以下区域推出：美国东部（弗吉尼亚州北部）、美国东部（俄亥俄州）、美国西部（俄勒冈州）、亚太地区（首尔）、亚太地区（新加坡）、亚太地区（悉尼）、亚太地区（东京）、欧洲地区（法兰克福）、欧洲地区（爱尔兰）、欧洲地区（伦敦）和欧洲地区（斯德哥尔摩）\n",
    "第一次查询的时候是因为调度和拉起资源的影响，一般第二次查询就会变快。 但过一段时间后，资源释放后这个问题又会出现。在资源拉起期间是不进行收费的。\n",
    "能支持TB/GB级数据的查询。 一般查询延迟为几十秒到几分钟。默认计算容量为32 CRPUs, 目前这个默认计算容量不可设置，但是roadmap中未来打算让用户可以进行设置。(Slack中Ryan 提到，如果引擎中任务有积压，它能够scale up）\n",
    "按照CRPU-hour单价进行计费，每个查询默认计算容量为32 CRPUs。 金额 = (0.125 hours x 32 CRPUs * $0.656 per CRPU-hour) , 有1分钟的最小计费时间。头12个月内，会有9CRPU hours的免费额度。\n",
    "AWS Clean Rooms 本身的workspace中无法查看，可以在AWS Billing/Bills 中查看Usage Quantity得到该信息。\n",
    "目前只支持S3，其他数据源近期没有具体计划。\n",
    "5个\n",
    "正在加入这些合规计划的进程中。\n",
    "对，目前必须在AWS上，而且必须是同一个region。\n",
    "需要数据的持有者，把数据上传到S3上，然后再用Glue爬去下，拿到表的schema。这样才能关联到AWS CleanRooms。\n",
    "对，只作用于分析，不能修改对方数据\n",
    "在联合分析获取他方数据时数据存在移动，但协作方的原始数据并不会存住在clean room内，clean room并不是一个物理存贮空间。\n",
    "是单方收费，只有查询的接收方会进行收费。\n",
    "会， Glue Data Catalog API 的调用也会被收费， 如果加密数据用了KMS-CMK也会被相应的收费。\n",
    "List 和Aggregation两种不同的分析规则下有区别， List 只能支持重合用户的，所以必须要有关联字段。Aggregation可以支持你仅仅去查询对方的数据，这种情况下，是可以不指定关联字段的。\n",
    "是的，就地就能加入clean room, 这个S3桶就是一般的S3桶，并没有任何特殊。但这个S3路径不能注册到AWS Lake Formation中。\n",
    "可以，可以参考代码 https://gitlab.aws.dev/rmalecky/aws-clean-rooms-notebooks/-/blob/main/single_collaborator_aggregation.ipynb\n",
    "目前不能\n",
    "目前5个参与方为最大限制，这个是软性的限制，slack频道中有披露最大支持的硬限制为10.\n",
    "C3R，是aws开源加密代码库。提供了C3R Client(一个可执行的Jar包)，目前仅支持对csv和parquet文件格式进行加密，后续可能会支持更多格式。 由于clean room把所有的字段分成三种类型： 指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column), 他们的加密方式有所不同，C3R client 会使用AES-GCM加密算法对sealed字段进行加密，会使用HMAC(Hash-based Message Authentication Code)来对fingerprint字段进行加密。\n",
    "由于C3R是客户端加密，所以clean room 关联S3中的数据已经是加密后的密文。\n",
    "首先Clean room 对于Server Side的加密是透明的，无需额外处理。Clean rooms 不支持S3的客户端加密，必须采用C3R客户端进行加密，加密完成以后把数据上传到S3桶，后续流程和不加密的流程是一致的。 加密这步需要比较多的手工操作，包括：* 加密前需要创建好collaboration(协作), 得到collaboration_id后续在加密中需要提供 * 利用openssl 生成32位密钥并分享给其他协作方。* 加密过程中需要指定哪些字段为指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column)\n",
    "(待补充)\n",
    "对于sealed和 fingerprint字段，只有string字段类型被支持。对于csv文件，C3R的客户端处理任何值都作为UTF-8编码的文本，加密前不会做任何其他的前置处理。对于parquet文件，对sealed和 fingerprint字段，如果出现非string的字段，会直接报错。C3R 客户端不能处理parquet中的复杂字段比如struct。\n",
    "不能，只能作为cleartext明文列\n",
    "基本都是标准化的算法，除了一个HKDF(一种密钥推导函数)的实现(来自RFC5869), 但是使用的是java标准加密库中的MAC算法。\n",
    "AMC是一个专门服务与Amazon Ads的clean room应用，它是并且将持续是唯一的服务于Amazon Ads客户的应用服务。AWS Clean Rooms 是一个云分析服务，会服务于各个行业的数据合作需求。2023年, AMC 将会把自己的查询引擎和计算基础设置迁移到AWS Clean Rooms服务，将会帮助AMC更方便的服务于客户(他们将不再需要把自己第一方数据上传到AMC，在AWS S3上即可使用).\n",
    "AWS Clean rooms 覆盖的客户范围更广。而其他的服务商客户范围相对小，需要把数据移动到他们的平台上。AWS Clean rooms则无需数据移动。(Bastion 2021年announce的时候，其他的云厂商比如(google cloud 和 microsoft azure) 还没有通用场景的clean room solution，snowflake 有，功能上有区别，只允许data provider 提供预先固定的SQL，Bastion灵活性更好。snowflake要求数据必须进入他们的数仓，aws在S3 即可)。\n",
    "\"\"\"\n",
    "answers=all_answer.split(\"\\n\")\n",
    "\n",
    "answer_2 = \"\"\"\n",
    "包含多个行业，下面提供部分参考\n",
    "* 广告营销领域：\n",
    "    * 需要进行广告营销活动，流量平台方需要给广告主或者营销方他们的广告点击数据和展现数据进行分析，但是不能提供用户级别的信息。\n",
    "    * 典型客户：营销方或广告主 P&G, Barclays，媒体-流量平台 Amazon Ads, Comcast, NBC Universal\n",
    "* 零售领域\n",
    "    * 银行和零售商需要获取他们的重叠用户，用于进行联合的市场活动，但不要把客户的其他信息暴露给彼此。（比如非场合用户）\n",
    "* 医疗健康领域 (结合之前的HCLS Slides， 并不是来自Bastion)\n",
    "    * 药厂和医院之间，药厂需要医院的病历数据； 药厂和外包的研发机构之间需要进行数据的共享。\n",
    "    * 典型客户：Change Healthcare, AstraZeneca \n",
    "* 其他领域的一些典型的客户\n",
    "    * 数据服务商 Foursquare ，Nielsen， IRI， \n",
    "    * 其他 Cars.com\n",
    "\"\"\"\n",
    "answers.append(answer_2)\n",
    "    \n",
    "all_answer3=\"\"\"都是利用了AWS Lake formation, AWS Clean Rooms 里 SQL中字段级别的限制约束，是通过一种new class of AWS Lake formation permission 来实现的。\n",
    "可以在clean room 的 workspace， 也可以在Redshift workspace （Note: 从目前发布产品文档上并没有，但是说明背后的引擎就是redshift severless)\n",
    "需要通过AWS Data Exchange 来进行 （Note: 目前AWS Clean Rooms并没有体现）\n",
    "AWS Clean Rooms 可以通过AWS Data Exchange 去浏览和寻找可用数据的合作方。 他是AWS Data Exchange的更近一步的服务，提供了可控(多种约束限制)和可审计的数据合作方式\n",
    "允许客户在clean room 创建视图，并且在AWS Clean Rooms中保存物化视图，一旦退出协作，AWS Lake formation permission 将会被撤销，这些物化视图会被删除。（Note: 目前AWS Clean Rooms并没有体现）\n",
    "目前这个版本不支持，后续的版本可能会考虑（NBC Universal 希望对于没有aws账号另外一方的数据可用）\n",
    "可以，可以利用query template来做（Note: 目前AWS Clean Rooms并没有体现）\n",
    "可以这么做，可以提供一些没有任何约束的示例数据给用户（Note: 目前AWS Clean Rooms并没有体现）\n",
    "它是一种live的共享，任何更新会立刻反映到联合分析的结果中\n",
    "\"\"\"\n",
    "answers=answers+(all_answer3.split(\"\\n\"))\n",
    "\n",
    "answer_4=\"\"\"\n",
    "主要有四个方向：\n",
    "1. Identity matching 身份ID对齐 (Note: 目前这项在官方的PPT在有体现)\n",
    "2. 对隐私攻击的防护， 有些查询即使是一些聚合分析，仍然可能探查到个人的信息\n",
    "    1. 限制访问同一块范围数据的query的数量\n",
    "    2. 采用差分隐私(Differential Privacy) 结果中添加噪声(会影响分析的精度)，Morgen Stanley 作为这个功能的beta用户\n",
    "3. 机器学习，P&G 表现出这方面的需求， 应该也是基于表格数据的模型，可能是流失预测，人群聚类等场景\n",
    "4. 和DSP的集成，直接把激活用户ID给到对接的DSP，而不通过cleanroom的数据接收方\n",
    "\"\"\"\n",
    "answers.append(answer_4)\n",
    "               \n",
    "answer_5=\"\"\"Horne, Bill <bgh@amazon.com>, Rababy, Bethany <rababyb@amazon.com>, Malecky, Ryan <rmalecky@amazon.com>, Malik, Mohsen <mmohsen@amazon.com>, Tanna, Shamir <tannas@amazon.com>\"\"\"\n",
    "answers.append(answer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8099c8-53e1-4581-9e87-381f5cf90020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"./code/\")\n",
    "#import func\n",
    "\n",
    "\n",
    "parameters = {\n",
    "      #\"early_stopping\": True,\n",
    "      #\"length_penalty\": 2.0,\n",
    "      \"max_new_tokens\": 50,\n",
    "      \"temperature\": 0,\n",
    "      \"min_length\": 10,\n",
    "      \"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "endpoint_name=\"st-paraphrase-mpnet-base-v2-2023-04-17-10-05-10-718-endpoint\"\n",
    "##########embedding by llm model##############\n",
    "sentense_vectors=get_vector_by_sm_endpoint(questions[0:5],sm_client,endpoint_name,parameters)\n",
    "sentense_vectors+=get_vector_by_sm_endpoint(questions[5:9],sm_client,endpoint_name,parameters)\n",
    "#sentense_vectors = get_vector_by_lanchain(questions , sm_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1af6a-3e60-4bc9-8610-9146a40a4924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(questions[0:5]))\n",
    "print(sentense_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a20d5-fdd0-4be2-835d-8afc753062a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for index, answer in enumerate(answers):\n",
    "    print(index, answer)\n",
    "    doc = {\n",
    "        \"question\":questions[index],\n",
    "        \"answer\": answer,\n",
    "        \"sentence_vector\": sentense_vectors[index]\n",
    "          }\n",
    "    docs.append(doc)\n",
    "#data = [{str(i): j for i, j in zip(['question', 'answer','sentence_vector'], values)} for values in zip(questions, answers,sentense_vectors)]\n",
    "#docs = json.dumps(data)\n",
    "print(docs)\n",
    "\n",
    "#########ingestion into aos ###################\n",
    "k_nn_ingestion_by_aos(docs,index,hostname,username,passwd)\n",
    "#k-nn_ingestion_by_lanchain(docs,opensearch_vector_search)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6fad1-bd2e-4070-8696-2c904d30910e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2:KNN search topN questio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4f8d6-1ece-40b0-a7bb-12cce30ddb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########2:KNN search topN question#########################\n",
    "query = \"\"\"what's the functions of cleaning room?\"\"\"\n",
    "query_embedding = get_vector_by_sm_endpoint(query,sm_client,endpoint_name,parameters)\n",
    "context = parse_results(search_using_aos_knn(query_embedding, hostname, username,passwd, \n",
    "                                           index, source_includes, size))\n",
    "#context = parse_results(search_using_lanchain(question, vectorSearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d17056-7e66-49d7-9e02-9e4869b92841",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3:warp up context ，construct prompt and call llm lanchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b36bb-54d5-48f3-a43d-f6e075f2f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########3:warp up context ，construct prompt and call llm lanchain######3\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=sm_llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "chain({\"input_documents\": NONE, \"question\": query, \"context\": context}, return_only_outputs=True)\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f3ccfac-ab97-48ad-ae79-c6465b3fe16e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OpenSearch([{'host': 'vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com', 'port': 443}])>\n"
     ]
    }
   ],
   "source": [
    "auth = ('admin', '(OL>0p;/')\n",
    "search = OpenSearch(\n",
    "         hosts = [{'host': hostname, 'port': 443}],\n",
    "         ##http_auth = awsauth ,\n",
    "         http_auth = auth ,\n",
    "         use_ssl = True,\n",
    "         verify_certs = True,\n",
    "         connection_class = RequestsHttpConnection\n",
    "     )\n",
    "print(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8d869-76e3-44e7-89c4-946983d6349a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
