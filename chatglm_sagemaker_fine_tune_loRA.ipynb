{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252de0de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SageMaker fine tune ChatGLM\n",
    "\n",
    "#### 准备\n",
    "1. 升级boto3, sagemaker python sdk  \n",
    "2. 准备inference.py, requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f2c403",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.26.71)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.116-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Collecting botocore<1.30.0,>=1.29.116\n",
      "  Downloading botocore-1.29.116-py3-none-any.whl (10.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.116->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.71\n",
      "    Uninstalling botocore-1.29.71:\n",
      "      Successfully uninstalled botocore-1.29.71\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.71\n",
      "    Uninstalling boto3-1.26.71:\n",
      "      Successfully uninstalled boto3-1.26.71\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.71 requires botocore==1.29.71, but you have botocore 1.29.116 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.116 botocore-1.29.116\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.148.0.tar.gz (743 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m743.3/743.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.26.116)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.116 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.116)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.148.0-py2.py3-none-any.whl size=998496 sha256=60972853c9019b0256fc91cbb9a01495e4e968902662ea7a9093c45a60210476\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/66/36/07/56de705f4ad8ea09e40419f57f8478bc60aae5b1e095dda1f0\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: cloudpickle, sagemaker\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.0\n",
      "    Uninstalling cloudpickle-2.2.0:\n",
      "      Successfully uninstalled cloudpickle-2.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.11.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cloudpickle-2.2.1 sagemaker-2.148.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a30f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker-us-west-2-687912291502\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f59e3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### chatglm lora方式（simple_thu_chatglm6b单机单卡）\n",
    "1: 使用羊驼语料数据  \n",
    "2：语料处理，tokenization及label标注  \n",
    "3：HF trainer API  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2f288-0610-4b18-9409-c8ef5ee91ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hikariming/alpaca_chinese_dataset.git\n",
    "!git clone https://github.com/yuanzhoulvpi2017/zero_nlp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bc6f4-152e-4697-9c5b-0174bb17b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os \n",
    "import pandas as pd \n",
    "import shutil\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f217e-c0c6-428b-ab10-50927e74c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir_list = ['alpaca_chinese_dataset/其他中文问题补充/',\n",
    "                   'alpaca_chinese_dataset/翻译后的中文数据/',\n",
    "                   'alpaca_chinese_dataset/chatglm问题数据补充/',\n",
    "                #    'alpaca_chinese_dataset/原始英文数据/'\n",
    "                   ]\n",
    "\n",
    "all_json_path = [glob(i+\"*.json\") for i in target_dir_list]\n",
    "all_json_path = list(chain(*all_json_path))\n",
    "len(all_json_path), all_json_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81470514-d789-47cc-9b2f-602bcb4f2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(x:str):\n",
    "    try:\n",
    "        data = pd.read_json(x)\n",
    "        return data \n",
    "    except Exception as e:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "alldata = pd.concat([read_json(i) for i in all_json_path])\n",
    "# alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118994f5-1b0f-4163-95ef-a1fef4b42002",
   "metadata": {},
   "outputs": [],
   "source": [
    "genrate_data_dir = \"data3_0328\"\n",
    "genrate_data_dir = Path(genrate_data_dir)\n",
    "\n",
    "if genrate_data_dir.exists():\n",
    "    shutil.rmtree(genrate_data_dir, ignore_errors=True)\n",
    "\n",
    "os.makedirs(genrate_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a128d-cbe3-4892-bbcc-1868e89d333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = alldata.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "chunk_size = 666\n",
    "\n",
    "for index, start_id in tqdm(enumerate(range(0, alldata.shape[0], chunk_size))):\n",
    "    temp_data = alldata.iloc[start_id:(start_id+chunk_size)]\n",
    "    temp_data.to_csv(genrate_data_dir.joinpath(f\"{index}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e3381-73c3-46be-8064-1b5490fa0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thuglm.modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "# from thuglmcode.model_chatglm import ChatGLMForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from typing import Optional\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4a012-3cd6-4c58-b382-0c6cf739ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yuanzhoulvpi/chatglm6b-dddd\", trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"yuanzhoulvpi/chatglm6b-dddd\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    # ['dense','dense_h_to_4h','dense_4h_to_h'] # 'query_key_value',\n",
    "    target_modules=['query_key_value',],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n",
    "        # If we are executing this function, we are the process zero, so we don't check for that.\n",
    "        output_dir = output_dir if output_dir is not None else self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        def save_tunable_parameters(model, path):\n",
    "            saved_params = {\n",
    "                k: v.to(\"cpu\") for k, v in model.named_parameters() if v.requires_grad\n",
    "            }\n",
    "            # saved_params = model.state_dict()\n",
    "            torch.save(saved_params, path)\n",
    "\n",
    "        save_tunable_parameters(\n",
    "            self.model, os.path.join(output_dir, \"chatglm-lora.pt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c1bbd-caaf-43fa-b3a8-8e74dfe44474",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "all_file_list = glob(pathname=genrate_data_dir.joinpath(\"*.csv\").__str__())\n",
    "\n",
    "test_file_list = random.sample(all_file_list, int(len(all_file_list)*0.25))\n",
    "train_file_list = [i for i in all_file_list if i not in test_file_list]\n",
    "\n",
    "len(train_file_list), len(test_file_list)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "    'train':train_file_list,\n",
    "    'valid':test_file_list\n",
    "    },\n",
    "    cache_dir=\"cache_data\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867116c-49fd-4ad9-a57e-04aef84b0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_and_position_ids(\n",
    "    seq, seq_len, context_length, device, gmask=False, position_encoding_2d=True\n",
    "):\n",
    "    mask_position = (\n",
    "        seq_len - 2\n",
    "    )  # is equal to `seq.index(mask_token)` or `seq.index(150001)`\n",
    "    attention_mask = torch.ones((1, context_length, context_length), device=device)\n",
    "    attention_mask.tril_()\n",
    "    attention_mask[..., : mask_position - 1] = 1\n",
    "    attention_mask = (attention_mask < 0.5).bool()\n",
    "\n",
    "    if position_encoding_2d:\n",
    "        seq_length = seq_len - 1  # is equal to `seq_length = seq.index(150004)`\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[seq_length:] = mask_position\n",
    "        block_position_ids = torch.cat(\n",
    "            (\n",
    "                torch.zeros(seq_length, dtype=torch.long, device=device),\n",
    "                torch.arange(\n",
    "                    context_length - seq_length, dtype=torch.long, device=device\n",
    "                )\n",
    "                + 1,\n",
    "            )\n",
    "        )\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
    "    else:\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[context_length - 1 :] = mask_position\n",
    "    return attention_mask, position_ids\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids) + 1\n",
    "    input_ids = []\n",
    "    attention_mask_list = []\n",
    "    position_ids_list = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1)\n",
    "            + ids[(seq_len - 1) :]\n",
    "            + [tokenizer.eop_token_id]\n",
    "            + [-100] * (longest - ids_l - 1)\n",
    "        )\n",
    "        ids = ids + [tokenizer.eop_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        attention_mask, position_ids = get_masks_and_position_ids(\n",
    "            ids, seq_len, longest, _ids.device, gmask=False\n",
    "        )\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        position_ids_list.append(position_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    attention_mask = torch.stack(attention_mask_list)\n",
    "    position_ids = torch.stack(position_ids_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"position_ids\": position_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3d501-8b24-4be0-a583-f95593914f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    # {\"context\": context, \"target\": target}\n",
    "    example['context'] = context\n",
    "    example['target'] = target\n",
    "    return example\n",
    "\n",
    "max_seq_length = 512\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True)\n",
    "    target_ids = tokenizer.encode(\n",
    "        target, max_length=max_seq_length, truncation=True, add_special_tokens=False\n",
    "    )\n",
    "    input_ids = prompt_ids + target_ids + [tokenizer.eos_token_id]\n",
    "    return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n",
    "\n",
    "def filter_nan(example):\n",
    "    return example['target'] is not None and example['context'] is not  None\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    function=format_example, remove_columns=dataset['train'].column_names\n",
    "    ).filter(function=filter_nan)\n",
    "tokenized_datasets = tokenized_datasets.map(function=preprocess)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f99b08-25d8-48ac-aafa-603744c3f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_callback import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "\n",
    "\n",
    "class EmptyCacheCallBack(TrainerCallback):\n",
    "    \"\"\"\n",
    "    通过callback的形式，解决显存不够的问题\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs, **kwargs):\n",
    "        \"\"\"\n",
    "        Event called after logging the last logs.\n",
    "        \"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def on_epoch_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def on_step_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "\n",
    "eccb = EmptyCacheCallBack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b17418-615c-4107-9868-aa915c27695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"test004\",\n",
    "    per_device_train_batch_size=2, \n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=100,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    # callbacks=[eccb]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff62875-5b26-47a8-8444-00da95780a51",
   "metadata": {},
   "source": [
    "### chatglm lora方式（单机多卡、模型并行）\n",
    "与单机单卡训练类似，使用pytorch ddp+lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 完整的训练流程\n",
    "# 1. 数据基于`https://github.com/hikariming/alpaca_chinese_dataset`\n",
    "# 2. 部分代码来源于`https://github.com/27182812/ChatGLM-chinese-insturct/blob/main/finetune.py`\n",
    "# 3. 基于我之前修改的`model_chatglm.py`做的一整套教程\n",
    "# \n",
    "# ## 清洗数据\n",
    "\n",
    "# 在这里控制要使用的显卡\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# 如果没有下载这个仓库，可以使用下面命令进行clone\n",
    "\n",
    "# !git clone https://github.com/hikariming/alpaca_chinese_dataset.git\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# part 1 数据准备\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "target_dir_list = ['alpaca_chinese_dataset/其他中文问题补充/',\n",
    "                   'alpaca_chinese_dataset/翻译后的中文数据/',\n",
    "                   'alpaca_chinese_dataset/chatglm问题数据补充/',\n",
    "                   #    'alpaca_chinese_dataset/原始英文数据/'\n",
    "                   ]\n",
    "\n",
    "all_json_path = [glob(i + \"*.json\") for i in target_dir_list]\n",
    "all_json_path = list(chain(*all_json_path))\n",
    "len(all_json_path), all_json_path[:5]\n",
    "\n",
    "\n",
    "def read_json(x: str):\n",
    "    try:\n",
    "        data = pd.read_json(x)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "alldata = pd.concat([read_json(i) for i in all_json_path])\n",
    "\n",
    "genrate_data_dir = \"data3_0328\"\n",
    "genrate_data_dir = Path(genrate_data_dir)\n",
    "\n",
    "if genrate_data_dir.exists():\n",
    "    shutil.rmtree(genrate_data_dir, ignore_errors=True)\n",
    "\n",
    "os.makedirs(genrate_data_dir, exist_ok=True)\n",
    "\n",
    "alldata = alldata.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "chunk_size = 666\n",
    "\n",
    "for index, start_id in tqdm(enumerate(range(0, alldata.shape[0], chunk_size))):\n",
    "    temp_data = alldata.iloc[start_id:(start_id + chunk_size)]\n",
    "    temp_data.to_csv(genrate_data_dir.joinpath(f\"{index}.csv\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# part 2 模型加载和转换\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "from thuglm.modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from typing import Optional\n",
    "import torch\n",
    "from MyTrainer import Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thuglm\", trust_remote_code=True)\n",
    "\n",
    "device_map_dict = {'transformer.word_embeddings': 0,\n",
    "                   'transformer.layers.0': 0,\n",
    "                   'transformer.layers.1': 0,\n",
    "                   'transformer.layers.2': 0,\n",
    "                   'transformer.layers.3': 0,\n",
    "                   'transformer.layers.4': 0,\n",
    "                   'transformer.layers.5': 0,\n",
    "                   'transformer.layers.6': 0,\n",
    "                   'transformer.layers.7': 0,\n",
    "                   'transformer.layers.8': 0,\n",
    "                   'transformer.layers.9': 0,\n",
    "                   'transformer.layers.10': 0,\n",
    "                   'transformer.layers.11': 0,\n",
    "                   'transformer.layers.12': 0,\n",
    "                   'transformer.layers.13': 0,\n",
    "                   'transformer.layers.14': 0,\n",
    "                   'transformer.layers.15': 1,\n",
    "                   'transformer.layers.16': 1,\n",
    "                   'transformer.layers.17': 1,\n",
    "                   'transformer.layers.18': 1,\n",
    "                   'transformer.layers.19': 1,\n",
    "                   'transformer.layers.20': 1,\n",
    "                   'transformer.layers.21': 1,\n",
    "                   'transformer.layers.22': 1,\n",
    "                   'transformer.layers.23': 1,\n",
    "                   'transformer.layers.24': 1,\n",
    "                   'transformer.layers.25': 1,\n",
    "                   'transformer.layers.26': 1,\n",
    "                   'transformer.layers.27': 1,\n",
    "                   'transformer.final_layernorm': 1,\n",
    "                   'lm_head': 1\n",
    "                   }\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"thuglm\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "for k, v in device_map_dict.items():\n",
    "    if k == 'transformer.word_embeddings':\n",
    "        model.transformer.word_embeddings = model.transformer.word_embeddings.to(f'cuda:{v}')\n",
    "    if k.find(\"transformer.layers\") != -1:\n",
    "        sub_value = int(k.replace(\"transformer.layers.\", \"\"))\n",
    "        model.transformer.layers[sub_value] = model.transformer.layers[sub_value].to(f'cuda:{v}')\n",
    "\n",
    "    if k == \"transformer.final_layernorm\":\n",
    "        model.transformer.final_layernorm = model.transformer.final_layernorm.to(f'cuda:{v}')\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    # ['dense','dense_h_to_4h','dense_4h_to_h'] # 'query_key_value',\n",
    "    target_modules=['query_key_value', ],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "for k, v in device_map_dict.items():\n",
    "    #     if k == 'transformer.word_embeddings':\n",
    "    #         model.base_model.transformer.word_embeddings = model.base_model.transformer.word_embeddings.to(f'cuda:{v}')\n",
    "    if k.find(\"transformer.layers\") != -1:\n",
    "        sub_value = int(k.replace(\"transformer.layers.\", \"\"))\n",
    "        model.base_model.transformer.layers[sub_value].attention.query_key_value.lora_A = \\\n",
    "            model.base_model.transformer.layers[sub_value].attention.query_key_value.lora_A.to(f'cuda:{v}')\n",
    "        model.base_model.transformer.layers[sub_value].attention.query_key_value.lora_B = \\\n",
    "            model.base_model.transformer.layers[sub_value].attention.query_key_value.lora_B.to(f'cuda:{v}')\n",
    "\n",
    "########################################################################################################################\n",
    "# part 3 数据加载\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "all_file_list = glob(pathname=genrate_data_dir.joinpath(\"*.csv\").__str__())\n",
    "\n",
    "test_file_list = random.sample(all_file_list, int(len(all_file_list) * 0.25))\n",
    "train_file_list = [i for i in all_file_list if i not in test_file_list]\n",
    "train_file_list, test_file_list = train_file_list[:5], test_file_list[:5]\n",
    "\n",
    "len(train_file_list), len(test_file_list)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        'train': train_file_list,\n",
    "        'valid': test_file_list\n",
    "    },\n",
    "    cache_dir=\"cache_data\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_masks_and_position_ids(\n",
    "        seq, seq_len, context_length, device, gmask=False, position_encoding_2d=True\n",
    "):\n",
    "    mask_position = (\n",
    "            seq_len - 2\n",
    "    )  # is equal to `seq.index(mask_token)` or `seq.index(150001)`\n",
    "    attention_mask = torch.ones((1, context_length, context_length), device=device)\n",
    "    attention_mask.tril_()\n",
    "    attention_mask[..., : mask_position - 1] = 1\n",
    "    attention_mask = (attention_mask < 0.5).bool()\n",
    "\n",
    "    if position_encoding_2d:\n",
    "        seq_length = seq_len - 1  # is equal to `seq_length = seq.index(150004)`\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[seq_length:] = mask_position\n",
    "        block_position_ids = torch.cat(\n",
    "            (\n",
    "                torch.zeros(seq_length, dtype=torch.long, device=device),\n",
    "                torch.arange(\n",
    "                    context_length - seq_length, dtype=torch.long, device=device\n",
    "                )\n",
    "                + 1,\n",
    "            )\n",
    "        )\n",
    "        position_ids = torch.stack((position_ids, block_position_ids), dim=0)\n",
    "    else:\n",
    "        position_ids = torch.arange(context_length, dtype=torch.long, device=device)\n",
    "        if not gmask:\n",
    "            position_ids[context_length - 1:] = mask_position\n",
    "    return attention_mask, position_ids\n",
    "\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids) + 1\n",
    "    input_ids = []\n",
    "    attention_mask_list = []\n",
    "    position_ids_list = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "                [-100] * (seq_len - 1)\n",
    "                + ids[(seq_len - 1):]\n",
    "                + [tokenizer.eos_token_id]\n",
    "                + [-100] * (longest - ids_l - 1)\n",
    "        )\n",
    "        ids = ids + [tokenizer.eos_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        attention_mask, position_ids = get_masks_and_position_ids(\n",
    "            ids, seq_len, longest, _ids.device, gmask=False\n",
    "        )\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        position_ids_list.append(position_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    attention_mask = torch.stack(attention_mask_list)\n",
    "    position_ids = torch.stack(position_ids_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"position_ids\": position_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    # {\"context\": context, \"target\": target}\n",
    "    example['context'] = context\n",
    "    example['target'] = target\n",
    "    return example\n",
    "\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    prompt = example[\"context\"]\n",
    "    target = example[\"target\"]\n",
    "    prompt_ids = tokenizer.encode(prompt, max_length=max_seq_length, truncation=True)\n",
    "    target_ids = tokenizer.encode(\n",
    "        target, max_length=max_seq_length, truncation=True, add_special_tokens=False\n",
    "    )\n",
    "    input_ids = prompt_ids + target_ids + [tokenizer.eos_token_id]\n",
    "    return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n",
    "\n",
    "\n",
    "def filter_nan(example):\n",
    "    return example['target'] is not None and example['context'] is not None\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    function=format_example, remove_columns=dataset['train'].column_names\n",
    ").filter(function=filter_nan)\n",
    "tokenized_datasets = tokenized_datasets.map(function=preprocess)\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# part 4 训练\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"modellog0040101\",\n",
    "    per_device_train_batch_size=4,  # 如果在24G显存上的显卡，可以开到4\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=20,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=20,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
