{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252de0de",
   "metadata": {},
   "source": [
    "### SageMaker  Endpoint 部署ChatGLM\n",
    "  \n",
    "[ChatGLM](https://github.com/THUDM/ChatGLM-6B): ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。\n",
    "\n",
    "#### 准备\n",
    "1. 升级boto3, sagemaker python sdk  \n",
    "2. 准备inference.py, requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f2c403",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (1.26.71)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.26.116-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3) (0.6.0)\n",
      "Collecting botocore<1.30.0,>=1.29.116\n",
      "  Downloading botocore-1.29.116-py3-none-any.whl (10.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3) (1.26.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.116->boto3) (1.16.0)\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.71\n",
      "    Uninstalling botocore-1.29.71:\n",
      "      Successfully uninstalled botocore-1.29.71\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.26.71\n",
      "    Uninstalling boto3-1.26.71:\n",
      "      Successfully uninstalled boto3-1.26.71\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.71 requires botocore==1.29.71, but you have botocore 1.29.116 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.26.116 botocore-1.29.116\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.132.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.148.0.tar.gz (743 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m743.3/743.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.26.116)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.4.4)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.116 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.116)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.116->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.148.0-py2.py3-none-any.whl size=998496 sha256=60972853c9019b0256fc91cbb9a01495e4e968902662ea7a9093c45a60210476\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/66/36/07/56de705f4ad8ea09e40419f57f8478bc60aae5b1e095dda1f0\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: cloudpickle, sagemaker\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.0\n",
      "    Uninstalling cloudpickle-2.2.0:\n",
      "      Successfully uninstalled cloudpickle-2.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.132.0\n",
      "    Uninstalling sagemaker-2.132.0:\n",
      "      Successfully uninstalled sagemaker-2.132.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.11.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cloudpickle-2.2.1 sagemaker-2.148.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a30f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker-us-west-2-687912291502\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region_name = boto3.session.Session().region_name\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2b0b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\n",
      "upload: ./model.tar.gz to s3://sagemaker-us-west-2-687912291502/chatglm/assets/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!touch dummy\n",
    "!tar czvf model.tar.gz dummy\n",
    "assets_dir = 's3://{0}/{1}/assets/'.format(bucket, 'chatglm')\n",
    "model_data = 's3://{0}/{1}/assets/model.tar.gz'.format(bucket, 'chatglm')\n",
    "!aws s3 cp model.tar.gz $assets_dir\n",
    "!rm -f dummy model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbcac2",
   "metadata": {},
   "source": [
    "### 设置模型推理参数\n",
    "1. model_name: Huggingface diffusers models (not support single check point format)\n",
    "2. model_args: diffuser StableDiffusionPipeline init arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b8a09a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = None\n",
    "entry_point = 'inference-chatglm.py'\n",
    "framework_version = '1.13.1'\n",
    "py_version = 'py39'\n",
    "model_environment = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT':'600', \n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d03288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model = PyTorchModel(\n",
    "    name = model_name,\n",
    "    model_data = model_data,\n",
    "    entry_point = entry_point,\n",
    "    source_dir = './code',\n",
    "    role = role,\n",
    "    framework_version = framework_version, \n",
    "    py_version = py_version,\n",
    "    env = model_environment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130efd9",
   "metadata": {},
   "source": [
    "### 部署 SageMaker Endpoint\n",
    "部署推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4253e7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = None\n",
    "instance_type = 'ml.g5.4xlarge'\n",
    "instance_count = 1\n",
    "\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor = model.deploy(\n",
    "    endpoint_name = endpoint_name,\n",
    "    instance_type = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer = JSONSerializer(),\n",
    "    deserializer = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f59e3f",
   "metadata": {},
   "source": [
    "### ChatGLM 测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#休眠2分钟,确保模型可以完全加载\n",
    "import time\n",
    "time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs= {\n",
    "    \"ask\": \"你好!\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"晚上睡不着应该怎么办\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edc6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs= {\n",
    "    \"ask\": \"列出一些年夜饭好意头的菜肴以及其寓意!\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"帮我写一篇人工智能课程的教案，1000字\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80082a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs= {\n",
    "    \"ask\": \"怎么修改huggingface transformers的model cache位置\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "inputs= {\n",
    "    \"ask\": \"用python3写出快速排序代码\"\n",
    "\n",
    "}\n",
    "\n",
    "response = predictor.predict(inputs)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ddf260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytorch-inference-2023-04-20-07-28-31-042'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#我们来查看一下刚部署的这个ChatGLM模型对应的SageMaker inference endpoint\n",
    "sagemaker_endpoint_name = predictor.endpoint_name\n",
    "sagemaker_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa257dda",
   "metadata": {},
   "source": [
    "利用已经在SageMaker real time inference endpoint部署的ChatGLM模型来模拟单轮对话和多轮对话。\n",
    "\n",
    "下面的代码建议在SageMaker Notebook上来运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078035cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json):\n",
    "    response = client.invoke_endpoint(EndpointName=sagemaker_endpoint_name, ContentType='application/json', Body=encoded_json)\n",
    "    return response\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    generated_text = model_predictions[\"answer\"]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f8af6b2b-3293-4e0b-ab92-71abef0a70d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pytorch-inference-2023-04-20-07-28-31-042'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7922fd8",
   "metadata": {},
   "source": [
    "先简单测试一下ChatGLM针对单个问题的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e4712-3159-43a4-b62d-a7b98490fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#payload = {\"ask\": \"信息抽取：\\n2022年世界杯的冠军是阿根廷队伍，梅西是MVP\\n问题：国家名，人名\\n答案：\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55a5b2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOOM的优化包括调整超参数和使用 warmup和降温策略。我们还尝试了不同的Adam参数和梯度裁剪方法，但没有取得明显效果。在重复数据上额外训练25B tokens以提高模型性能。\n"
     ]
    }
   ],
   "source": [
    "payload1 = {\"ask\": \"\"\"问题:GPU优化作用？\n",
    "                      回答:一般来说，GPU无法在检索数据同时执行这些计算。此外，现代GPU的计算性能远远高于每个操作(被称为GPU编程中的核)所需的内存传输速度。\n",
    "核融合是一种基于GPU计算的优化方法，通过在一次内核调用中执行多个连续操作。该方法提供了一种最小化数据传输的方法：中间结果留在GPU寄存器中，而不是复制到VRAM，从而节省开销。\n",
    "我们使用了Megatron-LM提供了几个定制化融合CUDA核。首先，我们使用一个优化核来执行LayerNorm，以及用核来融合各种缩放、掩码和softmax操作的各种组合。\n",
    "使用Pytorch的JIT功能将一个偏差项添加至GeLU激活中。作为一个使用融合核的例子，在GeLU操作中添加偏差项不会增加额外的时间，因为该操作受内存限制：与GPU VRAM和寄存器之间的数据传输相比，额外的计算可以忽略不计。\n",
    "因此融合这两个操作基本上减少了它们的运行时间\n",
    "                     问题：主题摘要,限制在10个字内\n",
    "           \"\"\"}\n",
    "\n",
    "payload2 ={ \"ask\": \"\"\"问题：如何优化BLOOM？\n",
    "                      回答：我们使用上表3中详细描述的超参数来训练BLOOM的6个尺寸变体。\n",
    "架构和超参数来自于我们的实验结果(Le Scao et al.)和先前的训练大语言模型(Brown et al.)。\n",
    "非176B模型的深度和宽度大致遵循先前的文献(Brown et al.)，偏离的3B和7.1B只是为了更容易适合我们训练设置。\n",
    "由于更大的多语言词表，BLOOM的embedding参数尺寸更大。在开发104B参数模型的过程中，我们使用了不同的Adam \n",
    "参数、权重衰减和梯度裁剪来对目标稳定性进行实验，但没有发现其有帮助。\n",
    "对于所有模型，我们在410B tokens使用cosine学习率衰减调度，在计算允许的情况下，将其作为训练长度的上限，并对375M tokens进行warmup。\n",
    "我们使用权重衰减、梯度裁剪，不使用dropout。ROOTS数据集包含341B tokens的文本。然而，基于训练期间发布的修订scaling laws，我们决定在重复数据上对大模型进行额外25B tokens的训练。\n",
    "由于warmup tokens + decay tokens大于总的token数量，所以学习率衰减始终未达到终点\n",
    "问题：主题摘要,限制在20个字内\n",
    "\"\"\"}\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload2).encode('utf-8'))\n",
    "generated_texts = parse_response_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75dba41a-725f-4f23-9379-cbaa5d6b81bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以上属于闲聊问题。 \n",
      "\n",
      "新加坡是一个旅游胜地，有许多值得探索的地方，例如：\n",
      "\n",
      "1. 新加坡河：一条历史悠久的河流穿过新加坡市中心，两岸是历史悠久的建筑物和漂亮的花园。\n",
      "\n",
      "2. 滨海湾金沙酒店：一家豪华酒店，位于滨海湾海滨，拥有壮丽的海景和豪华的设施。\n",
      "\n",
      "3. 圣淘沙岛：一个由多个岛屿组成的旅游胜地，岛上有许多主题公园、海滩、博物馆和餐馆。\n",
      "\n",
      "4. 新加坡动物园：一个大型的动物园，拥有许多珍稀的动物，包括亚洲象、熊猫、长颈鹿等。\n",
      "\n",
      "5. Sentosa Island：一个集娱乐、购物、餐饮和休闲于一体的综合性岛屿，有许多主题公园、海滩、博物馆和餐馆。\n",
      "\n",
      "除此之外，新加坡还有许多其他的景点和活动，例如滨海艺术中心、新加坡国家博物馆、水族馆、新加坡动物园等。\n"
     ]
    }
   ],
   "source": [
    "#payload={\"ask\":\"\"\"好累啊，想去米亚罗看红叶\n",
    "#问题：内容分类,以上属于闲聊还是专业问题？\"\"\"}\n",
    "\n",
    "payload={\"ask\":\"\"\"新加坡有什么好玩的么？\n",
    "问题：内容分类,以上是闲聊还是提问专业问题？\"\"\"}\n",
    "\n",
    "#payload={\"ask\":\"\"\"AWS是什么公司？\n",
    "#问题：内容分类,以上是闲聊还是提问专业问题？\"\"\"}\n",
    "query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "generated_texts = parse_response_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81504f9",
   "metadata": {},
   "source": [
    "## 单轮对话：每个问题/query都是独立的，问题之间没有关联性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82254c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.首先需要一个简单的开场拍。\n",
    "print(\"用户：你好！\\nChatGLM：您好!我是ChatGLM。我可以回答您的问题、写文章、写作业、翻译，对于一些法律等领域的问题我也可以给你提供信息。\")\n",
    "\n",
    "#2.在同一个session中持续对话，但是每次对话之间没有什么关联。\n",
    "while True:\n",
    "    command = input(\"用户：\")\n",
    "    if command == 'quit':\n",
    "        break;\n",
    "    \n",
    "    #print(command)\n",
    "    payload = {\"ask\": \"\\n用户：\"+ command + \"\\n\"}\n",
    "    #print(payload[\"ask\"])\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "    generated_texts = \"ChatGLM：\" + parse_response_texts(query_response)\n",
    "    print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326def35",
   "metadata": {},
   "source": [
    "## 多轮对话模拟：我们这里来测试一下ChatGLM的多轮对话能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.首先需要开场拍来引导ChatGLM，其实就是给它一个上下文来启动所谓的对话session。\n",
    "payload = {\"ask\": \"用户：你好！\\nChatGLM：您好!我是ChatGLM。我可以回答您的问题、写文章、写作业、翻译，对于一些法律等领域的问题我也可以给你提供信息。\"}\n",
    "print(payload[\"ask\"])\n",
    "generated_texts = \"\"\n",
    "\n",
    "#在这里简单模拟多轮对话时，发送给SageMaker endpoint的payload会越来越大，这里对payload大致做一个限制。\n",
    "session_len = 10 * 1024 * 1024 \n",
    "\n",
    "#2.在同一个session中持续对话，为了有多轮对话的效果，把每一轮的信息(问题-回答对)都带上来告诉ChatGLM之前的上下文。\n",
    "while True:\n",
    "    command = input(\"用户：\")\n",
    "    if command == 'quit':\n",
    "        break;\n",
    "    \n",
    "    #print(command)\n",
    "    whole_context = payload[\"ask\"] + generated_texts + \"\\n用户：\"+ command + \"\\n\"\n",
    "    payload = {\"ask\": whole_context}\n",
    "    if len(whole_context) > session_len:\n",
    "        print(\"上下文信息太多了，当前对话session要退出了！\")\n",
    "        break;\n",
    "    #print(payload[\"ask\"])\n",
    "    query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "    generated_texts = \"ChatGLM：\" + parse_response_texts(query_response)\n",
    "    print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2dcc6a",
   "metadata": {},
   "source": [
    "### 删除SageMaker  Endpoint\n",
    "删除推理服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c512b41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
